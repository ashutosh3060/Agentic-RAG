{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhweesBaFfzJZB70QVtDmM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashutosh3060/Agentic-RAG/blob/main/Overview_of_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Purpose of the notebook: This notebook is meant for basic RAG set up"
      ],
      "metadata": {
        "id": "kOmAL7_WItd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "### 0. Pre-requisites\n",
        "### 1. Overview of all the components of RAG\n",
        "### 2. Indexing\n",
        "### 3. Retrieval\n",
        "### 4. Generation"
      ],
      "metadata": {
        "id": "MCODimRKIneC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Pre-requisites"
      ],
      "metadata": {
        "id": "ZjQaYKFNJI8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 1: Install all the dependent libraries inside a virtual environment\n",
        "\n",
        "* Install virtual environment library\n",
        "* Create a virtual environment\n",
        "* Activate the environment\n",
        "* Install the required libraries from requirements.txt file\n",
        "\n",
        "Option 2: Install dependent libraries onn the global python environment\n"
      ],
      "metadata": {
        "id": "6nNE3RvsJUvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Option 1: Install all the dependent libraries inside a virtual environment"
      ],
      "metadata": {
        "id": "LkYITo2EO8tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install virtualenv\n",
        "!virtualenv \"rag_venv\" # To set up the env\n",
        "\n",
        "!source /content/rag_venv/bin/activate # To activate the environment and download all its dependencies and packages\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyAypWQuLgTS",
        "outputId": "d68cdea9-2f15-4a1c-f3f9-3fe35b97c58d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: virtualenv in /usr/local/lib/python3.11/dist-packages (20.29.2)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (0.3.9)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (3.17.0)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (4.3.6)\n",
            "created virtual environment CPython3.11.11.final.0-64 in 573ms\n",
            "  creator CPython3Posix(dest=/content/rag_venv, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==25.0.1, setuptools==75.8.2, wheel==0.45.1\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !source /content/myenv/bin/activate; pip3 list"
      ],
      "metadata": {
        "id": "frSuFk4qKBgY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! which python"
      ],
      "metadata": {
        "id": "QrUtkHqPL1_R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all packages\n",
        "! pip install -r /content/requirements.txt --quiet"
      ],
      "metadata": {
        "id": "L7vlI4gKMEb1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Option 2: Install dependent libraries onn the global python environment"
      ],
      "metadata": {
        "id": "1peZ8ypVO1uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install --quiet pinecone-client python-dotenv langchain langchain-community langchain-core langchain-openai beautifulsoup4 tiktoken numpy\n"
      ],
      "metadata": {
        "id": "X_KwZaXdNnxf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment"
      ],
      "metadata": {
        "id": "zNPVW3SdPdEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) Packages"
      ],
      "metadata": {
        "id": "d4M4JZ8NQC2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load all environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Access the environment variables (Langchain)\n",
        "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
        "langchain_endpoint = os.getenv('LANGCHAIN_ENDPOINT')\n",
        "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
        "\n",
        "# LLM\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Pinecone Vector Database\n",
        "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
        "pinecone_api_host = os.getenv('PINECONE_API_HOST')\n",
        "index_name = os.getenv('PINECONE_INDEX_NAME')"
      ],
      "metadata": {
        "id": "fF2i-PldP_-z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = langchain_tracing_v2\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = langchain_endpoint\n",
        "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
      ],
      "metadata": {
        "id": "-0VnggceUoX8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "#Pinecone keys\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "os.environ['PINECONE_API_HOST'] = pinecone_api_host\n",
        "os.environ['PINECONE_INDEX_NAME'] = index_name"
      ],
      "metadata": {
        "id": "5ZepXSNSUuQ_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hWjjtMUmM4fb",
        "outputId": "6c846d48-d6a0-4c6b-8c2c-83ad4cd4c3ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ragtest'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(langchain_tracing_v2)\n",
        "print(langchain_endpoint)\n",
        "print(langchain_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5d8n6CyUK4g",
        "outputId": "518b6f35-1044-43f4-a09b-c1c3e2bcef03"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true\n",
            "https://api.smith.langchain.com\n",
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pinecone Init"
      ],
      "metadata": {
        "id": "V-syaypBS1G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pinecone import Pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
        "index_name = \"ragtest\"  # change if desired\n",
        "\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "if index_name not in existing_indexes:\n",
        "  pc.create_index(\n",
        "      name=index_name,\n",
        "      dimension=1536,\n",
        "      metric=\"cosine\",\n",
        "      index_type=\"hnsw\",\n",
        "      sepc=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "  )\n",
        "  while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "    time.sleep(1)\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "XM5CIJtiSrLQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Overview of all the components of RAG"
      ],
      "metadata": {
        "id": "uCXsneBRPCmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all the libraries\n",
        "\n",
        "# For beautiful print\n",
        "from pprint import pprint\n",
        "#Beautiful Soup is a library that makes it easy to scrape information from web pages\n",
        "import bs4\n",
        "\n",
        "# Langchain Libraries\n",
        "# Taking inspiration from Hugging Face Hub, LangChainHub is collection of all artifacts useful for working with LangChain primitives such as prompts, chains and agents\n",
        "from langchain import hub\n",
        "# take a document and split into chunks that can be used for retrieval\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# loading documents from a variety of source\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "# Working with Pinecone vector store\n",
        "# from langchain_community.vectorstores import Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "# Langchain output parser\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# to passthrough inputs unchanged or with additional keys\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "# OpenAI model and embeddings\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDkZrHZkOsSU",
        "outputId": "21dd2c1b-d9ad-4555-cd17-a008bc6a58c6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Indexing"
      ],
      "metadata": {
        "id": "jixoqlvkRMOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Documents\n",
        "\n",
        "# loader = WebBaseLoader(\n",
        "#     web_paths = (\"https://greenly.earth/en-gb/blog/ecology-news/climate-change-in-2022-where-do-we-stand\",),\n",
        "#     bs_kwargs = dict(\n",
        "#         parse_only=bs4.SoupStrainer(\n",
        "#             class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "#             )\n",
        "#         )\n",
        "# )\n",
        "\n",
        "loader = WebBaseLoader(\"https://greenly.earth/en-gb/blog/ecology-news/climate-change-in-2022-where-do-we-stand\")\n",
        "\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "Dn1slbkKRP7r"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "Lp1PcyCdJ-Ot"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed\n",
        "\n",
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
        "    index_name=index_name\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "pSjwtyLVLnh3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Retrieval & Generation"
      ],
      "metadata": {
        "id": "yjtZrN4_TB4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "# prompt = hub.pull(\"rlm/rag-prompt:50442af1\") --Another prompt (check if it is the same, check quickly with results difference)"
      ],
      "metadata": {
        "id": "7N7xy8PmTEvC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.1) #Temperature is usually a value between 0 and 1, where 0 is completely deterministic and 1 is highly random"
      ],
      "metadata": {
        "id": "wT3iQH4_WjN_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post Processing\n",
        "def format_docs(docs):\n",
        "  '''\n",
        "  This function formats the retrieved output from the vectorstore as below.\n",
        "  Get the doc content and format it in such a way that there are 2 new lines between every doc content\n",
        "  '''\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "c8RXSJs4Xnbo"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain\n",
        "\n",
        "# In the below chain, first input is passed through RunnablePassthrough, then the retriever retrieves the information from vector store\n",
        "# The output from the vector store is post processed\n",
        "# The processed output from vector store is then fed to the prompt for prompt engineering or specific prompt styling\n",
        "# The LLM receives the prompt in the specified style and generates the text / answer.\n",
        "# The LLM's output is then formatted through Langchain's String Output parser and being returned.\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "qSJ8oUwjZXqM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Question\n",
        "# pprint(rag_chain.invoke(\"How does LangChain use vector stores for efficient data retrieval?\"))"
      ],
      "metadata": {
        "id": "Xk8weqJFmgrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question\n",
        "pprint(rag_chain.invoke(\"worst projection for climate change in 2024\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVUmjM1Tbw-w",
        "outputId": "cf25fbc8-7fdb-4a72-951f-69765514c7b1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('The worst projection for climate change in 2024 indicates a nearly 50% '\n",
            " 'chance that the average global temperature will rise above 1.5Â°C during the '\n",
            " 'five-year period from 2022 to 2026. This follows record-breaking '\n",
            " 'temperatures in 2023, suggesting that 2024 will continue to experience '\n",
            " 'exceptionally warm conditions. Overall, the outlook for climate change in '\n",
            " '2024 is concerning, with expectations of worsening impacts.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* End of Notebook"
      ],
      "metadata": {
        "id": "-z6zRhWihY6H"
      }
    }
  ]
}