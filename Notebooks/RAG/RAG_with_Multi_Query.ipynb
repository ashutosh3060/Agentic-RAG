{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashutosh3060/Agentic-RAG/blob/main/Notebooks/RAG/RAG_with_Multi_Query.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Purpose of the notebook:** This notebook is meant for multi-query RAG Architecture and working on it\n",
        "  * **What happens with a Multi-Query Translation ?**    \n",
        "  Ans: With multi-query translation, an AI system uses a Large Language Model (LLM) to generate multiple versions of a user's query, then retrieves documents for each version, and finally combines the unique documents to provide a richer, more comprehensive context for the LLM to generate a response"
      ],
      "metadata": {
        "id": "kOmAL7_WItd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "### 0. Pre-requisites\n",
        "### 1. Multi-Query RAG Architecture\n",
        "### 2.\n",
        "### 3.\n",
        "### 4."
      ],
      "metadata": {
        "id": "MCODimRKIneC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Pre-requisites"
      ],
      "metadata": {
        "id": "ZjQaYKFNJI8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 1: Install all the dependent libraries inside a virtual environment\n",
        "\n",
        "* Install virtual environment library\n",
        "* Create a virtual environment\n",
        "* Activate the environment\n",
        "* Install the required libraries from requirements.txt file\n",
        "\n",
        "Option 2: Install dependent libraries onn the global python environment\n"
      ],
      "metadata": {
        "id": "6nNE3RvsJUvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Option 1: Install all the dependent libraries inside a virtual environment"
      ],
      "metadata": {
        "id": "LkYITo2EO8tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install virtualenv\n",
        "!virtualenv \"rag_venv\" # To set up the env\n",
        "\n",
        "!source /content/rag_venv/bin/activate # To activate the environment and download all its dependencies and packages\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyAypWQuLgTS",
        "outputId": "f163b979-ce9e-44f9-9409-de93565a5a06"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: virtualenv in /usr/local/lib/python3.11/dist-packages (20.29.3)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (0.3.9)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (3.17.0)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (4.3.6)\n",
            "created virtual environment CPython3.11.11.final.0-64 in 603ms\n",
            "  creator CPython3Posix(dest=/content/rag_venv, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==25.0.1, setuptools==75.8.0, wheel==0.45.1\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !source /content/myenv/bin/activate; pip3 list"
      ],
      "metadata": {
        "id": "frSuFk4qKBgY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! which python"
      ],
      "metadata": {
        "id": "QrUtkHqPL1_R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all packages\n",
        "! pip install -r /content/requirements.txt --quiet"
      ],
      "metadata": {
        "id": "L7vlI4gKMEb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb0e32dd-d752-4ea9-c600-c267c71686fd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Option 2: Install dependent libraries onn the global python environment"
      ],
      "metadata": {
        "id": "1peZ8ypVO1uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install --quiet pinecone-client python-dotenv langchain langchain-community langchain-core langchain-openai beautifulsoup4 tiktoken numpy\n"
      ],
      "metadata": {
        "id": "X_KwZaXdNnxf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment"
      ],
      "metadata": {
        "id": "zNPVW3SdPdEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) Packages"
      ],
      "metadata": {
        "id": "d4M4JZ8NQC2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load all environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Access the environment variables (Langchain)\n",
        "langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
        "langchain_endpoint = os.getenv('LANGCHAIN_ENDPOINT')\n",
        "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
        "\n",
        "# LLM\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Pinecone Vector Database\n",
        "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
        "pinecone_api_host = os.getenv('PINECONE_API_HOST')\n",
        "index_name = os.getenv('PINECONE_INDEX_NAME')"
      ],
      "metadata": {
        "id": "fF2i-PldP_-z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = langchain_tracing_v2\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = langchain_endpoint\n",
        "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
      ],
      "metadata": {
        "id": "-0VnggceUoX8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "openai_model = \"gpt-3.5-turbo\"\n",
        "\n",
        "#Pinecone keys\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "os.environ['PINECONE_API_HOST'] = pinecone_api_host\n",
        "os.environ['PINECONE_INDEX_NAME'] = index_name"
      ],
      "metadata": {
        "id": "5ZepXSNSUuQ_"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hWjjtMUmM4fb",
        "outputId": "ef32d6cd-c25d-47ef-aab9-040782924baf"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ragtest'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(langchain_tracing_v2)\n",
        "print(langchain_endpoint)\n",
        "print(langchain_api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5d8n6CyUK4g",
        "outputId": "95041c9a-88a1-4555-b574-a0f96458f868"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true\n",
            "https://api.smith.langchain.com\n",
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pinecone Init"
      ],
      "metadata": {
        "id": "V-syaypBS1G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pinecone import Pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
        "index_name = \"ragtest\"  # change if desired\n",
        "\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "if index_name not in existing_indexes:\n",
        "  pc.create_index(\n",
        "      name=index_name,\n",
        "      dimension=1536,\n",
        "      metric=\"cosine\",\n",
        "      index_type=\"hnsw\",\n",
        "      sepc=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "  )\n",
        "  while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "    time.sleep(1)\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "XM5CIJtiSrLQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understand in Detail"
      ],
      "metadata": {
        "id": "22DqkO5r-E4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Index"
      ],
      "metadata": {
        "id": "uCXsneBRPCmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "# from langchain_community.vectorstores import Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pprint import pprint\n",
        "\n",
        "#### INDEXING ####\n",
        "\n",
        "# Load Document (Uploading one file at a time)\n",
        "pdf_file_path = \"/content/langchain_turing.pdf\"\n",
        "loader = PyPDFLoader(pdf_file_path)\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "# Upload muiltiple PDF files from a directory\n",
        "# pdf_file_paths = <enter your path here>\n",
        "# loader = PyPDFDirectoryLoader(pdf_file_paths)\n",
        "\n",
        "# docs_dir = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=500)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Index\n",
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
        "    index_name=index_name\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "iDkZrHZkOsSU"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Prompt"
      ],
      "metadata": {
        "id": "jixoqlvkRMOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Multi Query: Different Perspectives\n",
        "template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_perspectives\n",
        "    | ChatOpenAI(model_name=openai_model, temperature=0.1)\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ],
      "metadata": {
        "id": "iF8qHXKP89_M"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def get_unique_union(documents: list[list]):\n",
        "    \"\"\" Unique union of retrieved docs \"\"\"\n",
        "    # Flatten list of lists, and convert each Document to string\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    # Get unique documents\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    # Return\n",
        "    return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "# Retrieve\n",
        "question = \"How does LangChain leverage modular components like LangGraph, LangSmith, and LangServe to address challenges in building scalable and secure LLM-powered applications?\"\n",
        "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
        "docs = retrieval_chain.invoke({\"question\":question})\n",
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6Z3-TlV899U",
        "outputId": "36091ffd-278e-4c58-d2a7-035e0cd0e3b0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# docs"
      ],
      "metadata": {
        "id": "y1cvJCdk897O"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3LlMeQee89oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Documents\n",
        "\n",
        "# loader = WebBaseLoader(\n",
        "#     web_paths = (\"https://greenly.earth/en-gb/blog/ecology-news/climate-change-in-2022-where-do-we-stand\",),\n",
        "#     bs_kwargs = dict(\n",
        "#         parse_only=bs4.SoupStrainer(\n",
        "#             class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "#             )\n",
        "#         )\n",
        "# )\n",
        "\n",
        "loader = WebBaseLoader(\"https://greenly.earth/en-gb/blog/ecology-news/climate-change-in-2022-where-do-we-stand\")\n",
        "\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "Dn1slbkKRP7r"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "Lp1PcyCdJ-Ot"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed\n",
        "\n",
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"),\n",
        "    index_name=index_name\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "pSjwtyLVLnh3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Retrieval & Generation"
      ],
      "metadata": {
        "id": "yjtZrN4_TB4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "# prompt = hub.pull(\"rlm/rag-prompt:50442af1\") --Another prompt (check if it is the same, check quickly with results difference)"
      ],
      "metadata": {
        "id": "7N7xy8PmTEvC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.1) #Temperature is usually a value between 0 and 1, where 0 is completely deterministic and 1 is highly random"
      ],
      "metadata": {
        "id": "wT3iQH4_WjN_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post Processing\n",
        "def format_docs(docs):\n",
        "  '''\n",
        "  This function formats the retrieved output from the vectorstore as below.\n",
        "  Get the doc content and format it in such a way that there are 2 new lines between every doc content\n",
        "  '''\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "c8RXSJs4Xnbo"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain\n",
        "\n",
        "# In the below chain, first input is passed through RunnablePassthrough, then the retriever retrieves the information from vector store\n",
        "# The output from the vector store is post processed\n",
        "# The processed output from vector store is then fed to the prompt for prompt engineering or specific prompt styling\n",
        "# The LLM receives the prompt in the specified style and generates the text / answer.\n",
        "# The LLM's output is then formatted through Langchain's String Output parser and being returned.\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "qSJ8oUwjZXqM"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Question\n",
        "# pprint(rag_chain.invoke(\"How does LangChain use vector stores for efficient data retrieval?\"))"
      ],
      "metadata": {
        "id": "Xk8weqJFmgrf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question\n",
        "pprint(rag_chain.invoke(\"worst projection for climate change in 2024\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVUmjM1Tbw-w",
        "outputId": "3e88f00a-36c1-48bc-d6cc-9ba9edfeffc4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('The worst projection for climate change in 2024 indicates that there is '\n",
            " 'almost a 50% chance for the average global temperature to rise above 1.5°C '\n",
            " 'during the five-year period from 2022 to 2026. Additionally, 2024 is '\n",
            " 'expected to be exceptionally warm, following record-breaking temperatures in '\n",
            " '2023. This trend highlights the ongoing worsening of climate change due to '\n",
            " 'high greenhouse gas emissions.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* End of Notebook"
      ],
      "metadata": {
        "id": "-z6zRhWihY6H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XIT5IbSA4A2E"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}