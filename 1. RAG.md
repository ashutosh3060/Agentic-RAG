# RAG (Retrieval Augmented Generation)

### 1. What is RAG ?
**Ans:** **RAG stands for Retrieval-Augmented Generation**. It’s a technique used in natural language processing (NLP) where a model combines both retrieval-based and generative approaches to generate more accurate and informative responses.

Here’s how it works:

**Retrieval**: First, the model retrieves relevant information or documents from a large corpus, usually based on the input query.
**Augmentation**: Then, it uses this retrieved data to augment or inform the generation process.
**Generation**: Finally, the model generates a response, often using the retrieved information, ensuring the response is both relevant and contextually correct.
RAG is useful because it can help models provide more specific and fact-based answers, especially for tasks where the model might not have all the knowledge internally (e.g., specific data points, up-to-date events, etc.).

It’s often used in more advanced AI applications, like conversational agents, question-answering systems, and any context where accurate, contextually aware responses are needed.


### 2. Build a RAG APP with Langchain
Part 1 : https://python.langchain.com/v0.2/docs/tutorials/rag/
Part 2: https://python.langchain.com/docs/tutorials/qa_chat_history/

### 3. Topics in Building RAG
1. Indexing:    
   Types of Indexing: HNSW, LSH, Product Quantization, Multi Scale Tree Graph (MSTG)     
   How to implement HNSW indexing in different vector stores ?    
      Pinecone: By default hnsw indexing is there when we create an index.
3. Chunking :   
      https://www.analyticsvidhya.com/blog/2024/10/chunking-techniques-to-build-exceptional-rag-systems/   
      https://medium.com/@ronit.patil.16.2001/advanced-chunking-methods-in-retrieval-augmented-generation-rag-898f73d5efb2   
4. Re-ranking:     
   Types: Embedding-based rerankers, BERT-based rerankers, ColBERT reranker, Reinforcement learning rerankers, Hybrid rerankers, Cross-encoder and Bi-encoder models    
5. Knowledge Graph for RAG    
6. Query Expansion   
7. Optimizing Relevance and Quality in RAG Systems   
      Advanced filtering techniques   
            a. Metadata-based filtering   
            b. Context-based filtering    
      Context distillation    
8. Best Practices to follow:    
   a) No fixed sized chunking    
   b) ...   
9. Different types of RAG architecture:
   a) https://humanloop.com/blog/rag-architectures
10. RAG Evaluation    
    a) Evaluation metrics using DeepEval   


### 4. Vector Database for RAG ?
1. Pinecone
2. Chromadb
3. Weaviate
4. Elastic Search - For Large Scale
5. Other better option


Miscellaneous Topics:

What is invocation mode ?

* The specific method used to trigger the retrieval process and feed relevant information to the LLM when generating a response to a user query.    
* Essentially, it determines how the LLM accesses and utilizes external data sources to enrich its response based on the current context of the query.    
** Different types of invocation mode:
> Batched calls: Sending multiple queries to the language model at once to be processed together.  
> Async calls: allow sending multiple requests without waiting for each response individually   
> Streaming: receiving the model's output in chunks as it generates it, allowing for near-real-time feedback. Essentially, batching improves efficiency by grouping requests, async allows parallel processing, and streaming provides continuous output as the model generates it.   

### 5. Good Repo for End-to-End RAG Implementation   
a) https://github.com/ashutosh3060/bRAG-langchain     
    
### 6. Multi-Query RAG Architecture:   
   a) Multi-Query : Different Perspectives    
   b) RAG-Fusion     
   c) RAG Decomposition Architecture - (break down complex queries into simpler, manageable sub-queries. Each sub-query focuses on a specific part of the larger question and is sent to specialized retrievers or databases to gather precise information)        
      c.1) Answer Recusrsively -     
      c.2) Answer Individually - (take the answers from each individual query and pass it on directly to the LLM to generate a final answer given the previous answers as context)   
   d) Step-Back Method - (generating more abstract or higher-level questions rather than directly addressing the original query)    
   e) HyDE: Hypothetical Document Embedding - (Improve document indexing with HyDE)    


   


Resources:
https://www.datacamp.com/blog/rag-interview-questions
https://skphd.medium.com/interview-questions-and-answers-on-retrieval-augmented-generation-rag-f5fb7b5b8228
